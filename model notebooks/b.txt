# üì¶ Step 1: Install Required Packages
!pip install -q transformers sentence-transformers pandas tqdm

# üìö Step 2: Import Libraries
import pandas as pd
from transformers import pipeline
from sentence_transformers import SentenceTransformer, util
import torch
from tqdm import tqdm
from google.colab import files

# ‚öôÔ∏è Step 3: Upload and Load Review Dataset from Local
uploaded = files.upload()
filename = list(uploaded.keys())[0]

df = pd.read_csv(filename)
df = df.dropna(subset=['review_text'])
df = df.reset_index(drop=True)
if '__index_level_0__' in df.columns:
    df = df.drop(columns='__index_level_0__')

# ‚ö†Ô∏è Optional: Use a smaller sample for fast experimentation
df = df.sample(n=min(10000, len(df)), random_state=42)

# Load more accurate ABSA model
absa_pipe = pipeline(
    "text-classification",
    model="yangheng/deberta-v3-large-absa-v1.1",
    tokenizer="yangheng/deberta-v3-large-absa-v1.1",
    device=0  # GPU
)

# Remove noisy short reviews
df = df[df['review_text'].str.len() > 15]
dataset = Dataset.from_pandas(df)

# ABSA analysis
def extract_aspects(batch):
    results = absa_pipe(batch['review_text'], batch_size=4)  # Smaller batch size
    aspects = [r['label'].split("#[SEP]")[0].strip() for r in results]
    sentiments = [r['label'].split("#[SEP]")[-1].strip() for r in results]
    return {"aspect": aspects, "aspect_sentiment": sentiments}

start_time = time.time()
dataset = dataset.map(extract_aspects, batched=True, batch_size=4)
df = dataset.to_pandas()
print(f"‚úÖ ABSA processing completed in {time.time() - start_time:.2f} seconds")

# Step 6.1: Run ABSA on user review
user_review = "that camera cant shoot in night clearly"
user_absa_result = absa_pipe(user_review)[0]['label']

# Step 6.2: Safe split of aspect and sentiment
parts = user_absa_result.split("#[SEP]")
if len(parts) == 2:
    user_aspect = parts[0].strip().lower()
    user_sentiment = parts[1].strip().lower()
else:
    print("‚ö†Ô∏è Unexpected ABSA format. Falling back.")
    user_aspect = "quality"
    user_sentiment = "negative" if "not" in user_review.lower() else "positive"

print(f"User aspect: {user_aspect}, Sentiment: {user_sentiment}")
# Invert user sentiment to recommend positive alternatives for negative reviews
target_sentiment = "positive" if user_sentiment == "negative" else "negative"

# Filter dataset for matching aspect and opposite sentiment
filtered_df = df[
    (df['aspect'].str.lower().str.contains(user_aspect, na=False)) &
    (df['aspect_sentiment'].str.lower() == target_sentiment)
]

# If no match, fall back to all positive reviews
if filtered_df.empty:
    print("‚ö†Ô∏è No aspect-based positive match found. Using all positive reviews as fallback.")
    filtered_df = df[df['aspect_sentiment'].str.lower() == "positive"]
# Step 7.1: Create embeddings for user input and product reviews
user_embedding = sbert_model.encode(user_review, convert_to_tensor=True)

# Combine product + review for matching context
filtered_df['combo_text'] = filtered_df['product_title'] + ": " + filtered_df['review_text']

combo_embeddings = sbert_model.encode(
    filtered_df['combo_text'].tolist(),
    convert_to_tensor=True,
    batch_size=64,
    show_progress_bar=True
)

# Step 7.2: Compute similarity
filtered_df['similarity'] = util.pytorch_cos_sim(user_embedding, combo_embeddings).squeeze().tolist()

# Step 7.3: Recommend top N most relevant *positive* products
top_n = 10
recommendations = (
    filtered_df.sort_values(by='similarity', ascending=False)
    .drop_duplicates(subset='product_title')
    .head(top_n)
)

# Display final recommendations
print(f"\n‚úÖ Top {top_n} Better Product Recommendations for Negative Feedback:")
print(recommendations[['product_title', 'review_text', 'aspect', 'aspect_sentiment', 'similarity']])

